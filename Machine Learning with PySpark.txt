Machine Learning with pyspark


emp_df = spark.read.csv("/Users/subbuch/employee.txt", header=True)
>>> emp_df
DataFrame[id: string, last_name: string, email: string, gender: string, department: string, start_date: string, salary: string, job_title: string, region_id: string]


>>> emp_df.schema
StructType(List(StructField(id,StringType,true),StructField(last_name,StringType,true),StructField(email,StringType,true),StructField(gender,StringType,true),StructField(department,StringType,true),StructField(start_date,StringType,true),StructField(salary,StringType,true),StructField(job_title,StringType,true),StructField(region_id,StringType,true)))



>>> emp_df.printSchema()
root
 |-- id: string (nullable = true)
 |-- last_name: string (nullable = true)
 |-- email: string (nullable = true)
 |-- gender: string (nullable = true)
 |-- department: string (nullable = true)
 |-- start_date: string (nullable = true)
 |-- salary: string (nullable = true)
 |-- job_title: string (nullable = true)
 |-- region_id: string (nullable = true)

>>> emp_df.columns
['id', 'last_name', 'email', 'gender', 'department', 'start_date', 'salary', 'job_title', 'region_id']


>>> emp_df.take(5)
[Row(id='1', last_name="'Kelley'", email="'rkelley0@soundcloud.com'", gender="'Female'", department="'Computers'", start_date="'10/2/2009'", salary='67470', job_title="'Structural Engineer'", region_id='2'), Row(id='2', last_name="'Armstrong'", email="'sarmstrong1@infoseek.co.jp'", gender="'Male'", department="'Sports'", start_date="'3/31/2008'", salary='71869', job_title="'Financial Advisor'", region_id='2'), Row(id='3', last_name="'Carr'", email="'fcarr2@woothemes.com'", gender="'Male'", department="'Automotive'", start_date="'7/12/2009'", salary='101768', job_title="'Recruiting Manager'", region_id='3'), Row(id='4', last_name="'Murray'", email="'jmurray3@gov.uk'", gender="'Female'", department="'Jewelery'", start_date="'12/25/2014'", salary='96897', job_title="'Desktop Support Technician'", region_id='3'), Row(id='5', last_name="'Ellis'", email="'jellis4@sciencedirect.com'", gender="'Female'", department="'Grocery'", start_date="'9/19/2002'", salary='63702', job_title="'Software Engineer III'", region_id='7')]


>>> emp_df.count()
1000


emp_mgrs_df = emp_df.filter("salary >= 100000")
>>> emp_mgrs_df.count()
478
>>> emp_mgrs_df.select("salary").show()
+------+
|salary|
+------+
|101768|
|118497|
|108657|
|108093|
|121966|
|141139|
|106659|
|148952|
|109890|
|115274|
|144724|
|126103|
|144965|
|113507|
|120579|
|107222|
|125668|
|113857|
|108378|
|133424|
+------+
only showing top 20 rows



from pyspark.ml.feature import MinMaxScaler
from p>>> from pyspark.ml.linalg import Vectors
>>> feature_df = spark.createDataFrame([(1, Vectors.dense([10.0,10000,1.0]),),
... (2,Vectors.dense([20.0,30000,2.0]),),
... (3,Vectors.dense([30.0,40000.0,3.0]),)],["id","features"])
>>> feature_df.take(1)
[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]



 sfeatures_df.select("features","sfeatures").show()
+------------------+--------------------+
|          features|           sfeatures|
+------------------+--------------------+
|[10.0,10000.0,1.0]|       [0.0,0.0,0.0]|
|[20.0,30000.0,2.0]|[0.5,0.6666666666...|
|[30.0,40000.0,3.0]|       [1.0,1.0,1.0]|
+------------------+--------------------+



from pyspark.ml.feature import StandardScaler
>>> from pyspark.ml.linalg import Vectors
>>> features_df = spark.createDataFrame([
... (1, Vectors.dense([10.0,10000,1.0]),),
... (2, Vectors.dense([20.0,30000,2.0]),),
... (3, Vectors.dense([30.0,40000,3.0]),)],["id","features"])
>>> features_df.take(1
... )
[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]))]


stand_smodel = feature_stand_scaler.fit(features_df)

>>> stand_sfeatures_df = stand_smodel.transform(features_df)

>>> stand_sfeatures_df.take(1)
[Row(id=1, features=DenseVector([10.0, 10000.0, 1.0]), sfeatures=DenseVector([-1.0, -1.0911, -1.0]))]

>>> stand_sfeatures_df.show()
+---+------------------+--------------------+
| id|          features|           sfeatures|
+---+------------------+--------------------+
|  1|[10.0,10000.0,1.0]|[-1.0,-1.09108945...|
|  2|[20.0,30000.0,2.0]|[0.0,0.2182178902...|
|  3|[30.0,40000.0,3.0]|[1.0,0.8728715609...|
+---+------------------+--------------------+

>>> 
>>> from pyspark.ml.feature import Bucketizer

>>> splits = [-float("inf"),-10.0,0.0,10.0, float("inf")]

>>> b_data = [(-800.0,),(-10.5,),(-1.7,),(0.0,),(8.2,),(90.1,)]

>>> b_df = spark.createDataFrame(b_data,["features"])

>>> b_df.show()
+--------+
|features|
+--------+
|  -800.0|
|   -10.5|
|    -1.7|
|     0.0|
|     8.2|
|    90.1|
+--------+

bucketizer = Bucketizer(splits=splits, inputCol="features",outputCol="bfeatures")


bucketed_df = bucketizer.transform(b_df)
>>> bucketed_df.show()
+--------+---------+
|features|bfeatures|
+--------+---------+
|  -800.0|      0.0|
|   -10.5|      0.0|
|    -1.7|      1.0|
|     0.0|      2.0|
|     8.2|      2.0|
|    90.1|      3.0|
+--------+---------+


sent_token = Tokenizer(inputCol="Sntence",outputCol="words")



sent_token = Tokenizer(inputCol="sentence",outputCol="words")
>>> sent_tokenized_df = sent_token.transform(sentences_df)
>>> sent_tokenized_df.show()
+---+--------------------+--------------------+
| id|            sentence|               words|
+---+--------------------+--------------------+
|  1|This is an introd...|[this, is, an, in...|
|  2|MLlib includes cl...|[mllib, includes,...|
|  3|It also contains ...|[it, also, contai...|
+---+--------------------+--------------------+


from pyspark.ml.feature import HashingTF, IDF
>>> 
>>> sentences_df
DataFrame[id: bigint, sentence: string]

>>> sentences_df.take(1)
[Row(id=1, sentence='This is an introduction to Spark MLlib')]

>>> sent_tokenized_df.take(1)
[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'])]

>>> hashingTF = HashingTF(inputCol="words",outputCol="rawFeatures",numFeatures=20)

>>> sent_hfTF_df = hashingTF.transform(sent_tokenized_df)




sent_hfTF_df.take(1)
[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {1: 2.0, 5: 1.0, 6: 1.0, 8: 1.0, 12: 1.0, 13: 1.0}))]
>>> idf = IDF(inputCol="rawFeatures", outputCol="idf_features")
>>> idfModel = idf.fit(sent_hfTF_df)
>>> tfidf_df = idfModel.transform(sent_hfTF_df)
>>> tfidf_df.take(1)
[Row(id=1, sentence='This is an introduction to Spark MLlib', words=['this', 'is', 'an', 'introduction', 'to', 'spark', 'mllib'], rawFeatures=SparseVector(20, {1: 2.0, 5: 1.0, 6: 1.0, 8: 1.0, 12: 1.0, 13: 1.0}), idf_features=SparseVector(20, {1: 0.5754, 5: 0.6931, 6: 0.6931, 8: 0.2877, 12: 0.0, 13: 0.2877}))]
